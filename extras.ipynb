{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning using HyperBand\n",
    "\n",
    "[HyperBand](https://github.com/zygmuntz/hyperband) is relatively a new method for tuning hyperparameters of iterative algorithms. What we want to do here is test a bunch of configurations for Random Forest and Gradient Boosting models.\n",
    "    The fundamental behind Hyperband is that it runs every config for 1 iteration to get an idea of how the algorithm learns and which hyperparameters helps it improve. Then it takes the best config and runs it for longer. Thats all Hyperband does: \"run random configurations on a specific schedule of iterations per configuration, using earlier results to select candidates for longer runs.\"\n",
    "    \n",
    " It starts with 81 runs, one iteration each. Then the best 27 configurations get three iterations each. Then the best nine get nine, and so on. After all runs are complete, the algorithm returns a best configuration found so far and you can run it all over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our Hyperband functions\n",
    "import sys\n",
    "sys.path.insert(0, './helpers')\n",
    "from hyperband import *\n",
    "\n",
    "!python3 ./helpers/hyperband/main_regression.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
